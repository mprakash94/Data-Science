{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All necessary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from urllib.request import urlopen\n",
    "import os\n",
    "import string\n",
    "import glob\n",
    "import shutil\n",
    "import csv\n",
    "import pprint\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from zipfile import ZipFile\n",
    "from urllib.request import urlopen\n",
    "from tempfile import NamedTemporaryFile\n",
    "from shutil import unpack_archive\n",
    "from io import BytesIO\n",
    "import logging\n",
    "from boto.s3.key import Key\n",
    "import ntpath\n",
    "import zipfile\n",
    "import sys\n",
    "from os.path import basename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here I created handler, formatter, loglevel etc..\n",
    "ErrorLog = 'errorLoggingP2.log'\n",
    "log = logging.basicConfig(filename=ErrorLog,level=logging.DEBUG, format='%(asctime)s- %(levelname)s %(message)s', datefmt='%m/%d/%Y %I:%M:%S %p', filemode = 'w')#ch1 = logging.FileHandler('ErrorLog') #output the logs to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = os.path.dirname(os.getcwd()) \n",
    "cur_path =  directory + '/Part_2'\n",
    "newpath = '/Users/sonalichaudhari/Desktop/FALL2017/ADS/Assignment_1/Part_2/LogCSV'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "argLen=len(sys.argv)\n",
    "year=''\n",
    "AWS_ACCESS_KEY_ID =''\n",
    "AWS_SECRET_ACCESS_KEY =''\n",
    "\n",
    "for i in range(1,len(sys.argv)):\n",
    "    val=sys.argv[i]\n",
    "    if val.startswith('year='):\n",
    "        pos=val.index(\"=\")\n",
    "        year=val[pos+1:len(val)]\n",
    "        continue\n",
    "    elif val.startswith('accessKey='):\n",
    "        pos=val.index(\"=\")\n",
    "        AWS_ACCESS_KEY_ID=val[pos+1:len(val)]\n",
    "        continue\n",
    "    elif val.startswith('secretKey='):\n",
    "        pos=val.index(\"=\")\n",
    "        AWS_SECRET_ACCESS_KEY=val[pos+1:len(val)]\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "if not AWS_ACCESS_KEY_ID or not AWS_SECRET_ACCESS_KEY:\n",
    "    logging.warning('no access or secret key provided')\n",
    "    print('No access or secret key provided')\n",
    "    exit()\n",
    "try:\n",
    "    conn = boto.connect_s3(AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY)\n",
    "    logging.info(\"Successfully connected to Amazon S3\")\n",
    "\n",
    "except:\n",
    "    logging.info(\"Invalid amazon keys\")\n",
    "    print(\"Invalid amazon keys\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading the files in zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unzip and downloading the files in the folder\n",
    "\n",
    "def openUnzip(url):\n",
    "    try:\n",
    "        if not os.path.exists(newpath):\n",
    "            os.makedirs(newpath)\n",
    "            with urlopen(url) as zipresp:\n",
    "                with ZipFile(BytesIO(zipresp.read())) as zfile:\n",
    "                    zfile.extractall(newpath)\n",
    "        else:\n",
    "            with urlopen(url) as zipresp:\n",
    "                with ZipFile(BytesIO(zipresp.read())) as zfile:\n",
    "                    zfile.extractall(newpath)\n",
    "    except:\n",
    "        logging.warning(\"url:\" + url + \" was not correct. Check the inputted year\")         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#year = \n",
    "if(year is None):\n",
    "     logging.warning(\"year was left blank. Assigning the year 2005 automatically\")\n",
    "else:\n",
    "     logging.info(\"year was set properly\")\n",
    "zipurl = 'http://www.sec.gov/dera/data/Public-EDGAR-log-file-data/'\n",
    "generalUrl = \"http://www.sec.gov/dera/data/PublicEDGAR-log-file-data/\"\n",
    "months = ['01',\"02\",\"03\",\"04\",\"05\",\"06\",\"07\",\"08\",\"09\",\"10\",\"11\",\"12\"]\n",
    "for m in range(len(months)):\n",
    "    if(str(months[m]) is \"01\" or str(months[m]) is \"02\" or str(months[m]) is \"03\"):\n",
    "        urlAppended = zipurl + str(year) +\"/Qtr1/\"+\"log\"+str(year)+str(months[m])+\"01.zip\"\n",
    "        #print(urlAppended)\n",
    "        openUnzip(urlAppended)\n",
    "    if(str(months[m]) is \"04\" or str(months[m]) is \"05\" or str(months[m]) is \"06\"):\n",
    "        urlAppended = zipurl + str(year) +\"/Qtr2/\"+\"log\"+str(year)+str(months[m])+\"01.zip\"\n",
    "        #print(urlAppended)\n",
    "        #openUnzip(urlAppended)\n",
    "    if(str(months[m]) is \"07\" or str(months[m]) is \"08\" or str(months[m]) is \"09\"):\n",
    "        urlAppended = zipurl + str(year) +\"/Qtr3/\"+\"log\"+str(year)+str(months[m])+\"01.zip\"\n",
    "        #print(urlAppended)\n",
    "        #openUnzip(urlAppended)\n",
    "    if(str(months[m]) is \"10\" or str(months[m]) is \"11\" or str(months[m]) is \"12\"):\n",
    "        urlAppended = zipurl + str(year) +\"/Qtr4/\"+\"log\"+str(year)+str(months[m])+\"01.zip\"\n",
    "        #print(urlAppended)\n",
    "        #openUnzip(urlAppended)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the CSV in a dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading all CSVs and storing it in a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_log_files = glob.glob(newpath+'/*.csv')\n",
    "dict_csv = {}\n",
    "for file_ in all_log_files:\n",
    "    f_name = os.path.basename(file_)\n",
    "    dict_csv[f_name] = pd.read_csv(file_,index_col=None, header=0)\n",
    "\n",
    "logging.info(\"Each csv has been read into a dataframe \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Data Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key,value in dict_csv.items():\n",
    "    df = dict_csv[key]\n",
    "    \n",
    "    #Dropping Cik,ip and accesion if NAN\n",
    "    df.dropna(subset=['cik'])\n",
    "    df.dropna(subset=['accession'])\n",
    "    df.dropna(subset=['ip'])\n",
    "    \n",
    "    # df.isnull().sum()\n",
    "\n",
    "    #Extension : if only extension is there, appending accession before the extension\n",
    "    df['extention'] = np.where(str(df['extention']).find('.') ==0,df['extention'],df['accession']+df['extention'])\n",
    "    logging.info(\"replaced all extensions with no extension name in front of . with accession number\")\n",
    "    df['extention'] = df['extention'].replace(np.nan, 'Unavailable', regex=True)\n",
    "    logging.info(\"replaced all extensions with NAN by 'Unavailable'\")\n",
    "\n",
    "    #Zone : if NAN fill it with the max used zone value\n",
    "    df['zone'] = df['zone'].fillna(df['ip'].value_counts().idxmax())\n",
    "    logging.info(\"replaced all zones with NAN with mazimum count of ip address\")\n",
    "    \n",
    "    #IDX : if NAN fill it with the max used idx value\n",
    "    df['idx'] = df['idx'].fillna(df['idx'].value_counts().idxmax())\n",
    "    logging.info(\"replaced all idx with NAN with value that max count\")\n",
    "    \n",
    "    #Norefer : if NAN fill it with the max used norefer value\n",
    "    df['norefer'] = df['norefer'].fillna(df['norefer'].value_counts().idxmax())\n",
    "    logging.info(\"replaced all norefer with NAN with value that max count\")\n",
    "\n",
    "    #Noagent : if NAN fill it with the max used noagent value\n",
    "    df['noagent'] = df['noagent'].fillna(df['noagent'].value_counts().idxmax())\n",
    "    logging.info(\"replaced all noagent with NAN with value that max count\")\n",
    "\n",
    "    #Crawler : if NAN fill it with the max used crawler value\n",
    "    df['crawler'] = df['crawler'].fillna(df['crawler'].value_counts().idxmax())\n",
    "    logging.info(\"replaced all crawler with NAN with value that max count\")\n",
    "\n",
    "    #Replace size column which has NAN with the mean file size\n",
    "    df['size'] = df['size'].fillna(df['size'].mean(axis=0))\n",
    "    logging.info(\"replaced all size column with NAN with value the mean value\")\n",
    "    # df['date'].notnull().value\n",
    "    # #sub_df.iloc[0]['A']\n",
    "\n",
    "    #Browser : if NAN fill it with the max used browswe value\n",
    "    df['browser'] = df['browser'].fillna(df['browser'].value_counts().idxmax())\n",
    "    logging.info(\"replaced allbrowser column with NAN with the common/most used browser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SUMMARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean File size\n",
    "#df['size_mean'] = df['size'].mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique IPs in the given month\n",
    "#df['ip_unique'] = df['ip'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unique CIK\n",
    "#df['cik'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unique Accession Number\n",
    "#df['accession'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Merging all the dataframe \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zipping the CSVs and the summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zipping(directory):\n",
    " \n",
    "    # initializing empty file paths list\n",
    "    file_paths = []\n",
    "    for root, directories, files in os.walk(directory):\n",
    "        for filename in files:\n",
    "            if filename.endswith('.csv'):\n",
    "                filepath = os.path.join(root, filename)\n",
    "                file_paths.append(filepath)\n",
    "                \n",
    "    with ZipFile('LogFiles.zip','w') as zip:\n",
    "        for file in file_paths:\n",
    "            #print(file)\n",
    "            zip.write(file, basename(file))\n",
    " \n",
    "    print('All files zipped successfully!')   \n",
    "    logging.info(\"All files zipped successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/sonalichaudhari/Desktop/FALL2017/ADS/Assignment_1/Part_2/LogCSV/log20040101.csv\n",
      "/Users/sonalichaudhari/Desktop/FALL2017/ADS/Assignment_1/Part_2/LogCSV/log20040201.csv\n",
      "/Users/sonalichaudhari/Desktop/FALL2017/ADS/Assignment_1/Part_2/LogCSV/log20040301.csv\n",
      "/Users/sonalichaudhari/Desktop/FALL2017/ADS/Assignment_1/Part_2/LogCSV/log20040401.csv\n",
      "/Users/sonalichaudhari/Desktop/FALL2017/ADS/Assignment_1/Part_2/LogCSV/log20040501.csv\n",
      "/Users/sonalichaudhari/Desktop/FALL2017/ADS/Assignment_1/Part_2/LogCSV/log20040601.csv\n",
      "/Users/sonalichaudhari/Desktop/FALL2017/ADS/Assignment_1/Part_2/LogCSV/log20040701.csv\n",
      "/Users/sonalichaudhari/Desktop/FALL2017/ADS/Assignment_1/Part_2/LogCSV/log20040801.csv\n",
      "/Users/sonalichaudhari/Desktop/FALL2017/ADS/Assignment_1/Part_2/LogCSV/log20040901.csv\n",
      "/Users/sonalichaudhari/Desktop/FALL2017/ADS/Assignment_1/Part_2/LogCSV/log20041001.csv\n",
      "/Users/sonalichaudhari/Desktop/FALL2017/ADS/Assignment_1/Part_2/LogCSV/log20041101.csv\n",
      "/Users/sonalichaudhari/Desktop/FALL2017/ADS/Assignment_1/Part_2/LogCSV/log20041201.csv\n",
      "All files zipped successfully!\n"
     ]
    }
   ],
   "source": [
    "zipping(newpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Uploading the zip to Amazon S3\n",
    "try:   \n",
    "    bucket_name = AWS_ACCESS_KEY_ID.lower()+str(st).replace(\" \", \"\").replace(\"-\", \"\").replace(\":\",\"\").replace(\".\",\"\")\n",
    "    bucket = conn.create_bucket(bucket_name,  location=s3.connection.Location.DEFAULT)\n",
    "    #zipfile = 'Problem1.zip'\n",
    "    logging.info(\"Uploading to Amazon S3\", zipfile, bucket_name)\n",
    "    \n",
    "    def percent_cb(complete, total):\n",
    "        sys.stdout.write('.')\n",
    "        sys.stdout.flush()\n",
    "    \n",
    "    k = Key(bucket)\n",
    "    k.key = 'Assign1Problem1'\n",
    "    k.set_contents_from_filename(zipfile,\n",
    "        cb=percent_cb, num_cb=10)\n",
    "    logging.into(\"Zip File successfully uploaded to S3\")\n",
    "except:\n",
    "    logging.info(\"invalid amazon keys\")\n",
    "    print(\"invalid amazon keys\")\n",
    "    exit()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
